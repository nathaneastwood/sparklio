% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/write.R
\name{write_to_spark}
\alias{write_to_spark}
\title{Write data from R to a spark connection}
\usage{
write_to_spark(what, sc, name, ..., overwrite = TRUE, cache_results = TRUE)
}
\arguments{
\item{what}{\code{character(1)}. A SQL query.}

\item{sc}{A \code{spark_connection}.}

\item{name}{\code{character(1)}. The name of the resulting table in Spark.}

\item{...}{Additional parameters to be passed to \code{\link[sparklyr:spark_read_csv]{sparklyr::spark_read_csv()}}. Not used for queries.}

\item{overwrite}{\code{logical(1)}. Overwrite an existing table (default: \code{TRUE})?}

\item{cache_results}{\code{logical(1)}. Cache the results of a query in Spark (default: \code{TRUE})?}
}
\value{
A \code{tbl_spark}. A pointer to the Spark DataFrame.
}
\description{
Either write a table to Spark or write the results of a query to Spark.
}
\details{
Given a SQL query, this function will execute it within a Spark context and then cache the results of that query in
a Spark DataFrame. This caching is the default to match the behaviour of uploading a table into Spark memory.
}
\seealso{
\code{\link[sparklyr:tbl_cache]{sparklyr::tbl_cache()}}, \code{\link[sparklyr:spark_read_csv]{sparklyr::spark_read_csv()}}, \code{\link[sparklyr:sdf_register]{sparklyr::sdf_register()}}
}
